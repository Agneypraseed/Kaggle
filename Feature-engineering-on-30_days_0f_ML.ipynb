{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-28T07:00:21.526730Z","iopub.execute_input":"2021-08-28T07:00:21.527138Z","iopub.status.idle":"2021-08-28T07:00:21.891477Z","shell.execute_reply.started":"2021-08-28T07:00:21.527053Z","shell.execute_reply":"2021-08-28T07:00:21.890604Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"### Scalar Standardization for numerical values\n\n# data = pd.read_csv(\"../input/30daysofmlfolds/train_folds.csv\")\n# test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n# useful_features = [c for c in data.columns if c not in (\"id\", \"target\", \"kfold\")]\n# categorical_cols_one_hot = [cname for cname in data.columns if data[cname].nunique() < 10 and \n#                         data[cname].dtype == \"object\"]\n# categorical_cols_ordinal = [col for col in data.columns if 'cat' in col and \n#                             col not in categorical_cols_one_hot]\n# numerical_cols = [col for col in useful_features if 'cont' in col]\n\n# test = test[useful_features]\n\n# final_predictions = []\n# scores = []\n\n# for fold in range(5):\n#     X_train = data[data.kfold != fold].reset_index(drop = True)\n#     X_valid = data[data.kfold == fold].reset_index(drop = True)\n#     X_test  = test.copy()\n    \n#     y_train = X_train.target\n#     y_valid = X_valid.target\n    \n#     X_train = X_train[useful_features]\n#     X_valid = X_valid[useful_features]  \n    \n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     OH_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n#     Scaler = preprocessing.StandardScaler()\n    \n#     X_train[categorical_cols_ordinal] = ordinal_encoder.fit_transform(X_train[categorical_cols_ordinal])\n#     X_valid[categorical_cols_ordinal] = ordinal_encoder.transform(X_valid[categorical_cols_ordinal])\n#     X_test[categorical_cols_ordinal] = ordinal_encoder.transform(X_test[categorical_cols_ordinal])\n    \n#     OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_cols_one_hot]))\n#     OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[categorical_cols_one_hot]))\n#     OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_cols_one_hot]))    \n#     # One-hot encoding removed index; put it back\n#     OH_cols_train.index = X_train.index\n#     OH_cols_valid.index = X_valid.index\n#     OH_cols_test.index = X_test.index    \n#     # Remove categorical columns (will replace with one-hot encoding)\n#     num_X_train = X_train.drop(categorical_cols_one_hot, axis=1)\n#     num_X_valid = X_valid.drop(categorical_cols_one_hot, axis=1)\n#     num_X_test = X_test.drop(categorical_cols_one_hot, axis=1)    \n#     # Add one-hot encoded columns to numerical features\n#     OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n#     OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n#     OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)    \n#     X_train = OH_X_train\n#     X_valid = OH_X_valid\n#     X_test = OH_X_test   \n    \n#     X_train[numerical_cols] = Scaler.fit_transform(X_train[numerical_cols])\n#     X_valid[numerical_cols] = Scaler.transform(X_valid[numerical_cols])\n#     X_test[numerical_cols] = Scaler.transform(X_test[numerical_cols])\n    \n#     model= XGBRegressor(n_estimators = 1000,learning_rate =0.05 , max_depth = 5 ,colsample_bytree=0.5,reg_alpha=15,random_state=42,tree_method = 'gpu_hist',gpu_id=0,predictor=\"gpu_predictor\")    \n#     model.fit(X_train, y_train,early_stopping_rounds=5, \n#              eval_set=[(X_valid, y_valid)], \n#              verbose=False)\n#     preds_valid = model.predict(X_valid)\n#     test_preds = model.predict(X_test)\n#     final_predictions.append(test_preds)\n#     rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n#     print(fold, rmse)\n#     scores.append(rmse)\n\n# print(np.mean(scores), np.std(scores))\n\n# 0 0.7192059959651127\n# 1 0.7190201677007514\n# 2 0.7211280496778217\n# 3 0.7211213825574007\n# 4 0.7198784978183463\n# 0.7200708187438865 0.0009066636604014239","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:00:21.893829Z","iopub.execute_input":"2021-08-28T07:00:21.894108Z","iopub.status.idle":"2021-08-28T07:00:21.898776Z","shell.execute_reply.started":"2021-08-28T07:00:21.894082Z","shell.execute_reply":"2021-08-28T07:00:21.897944Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"### Normalization of numerical values\n\n# data = pd.read_csv(\"../input/30daysofmlfolds/train_folds.csv\")\n# test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n# useful_features = [c for c in data.columns if c not in (\"id\", \"target\", \"kfold\")]\n# categorical_cols_one_hot = [cname for cname in data.columns if data[cname].nunique() < 10 and \n#                         data[cname].dtype == \"object\"]\n# categorical_cols_ordinal = [col for col in data.columns if 'cat' in col and \n#                             col not in categorical_cols_one_hot]\n# numerical_cols = [col for col in useful_features if 'cont' in col]\n\n# test = test[useful_features]\n\n# final_predictions = []\n# scores = []\n\n# for fold in range(5):\n#     X_train = data[data.kfold != fold].reset_index(drop = True)\n#     X_valid = data[data.kfold == fold].reset_index(drop = True)\n#     X_test  = test.copy()\n    \n#     y_train = X_train.target\n#     y_valid = X_valid.target\n    \n#     X_train = X_train[useful_features]\n#     X_valid = X_valid[useful_features]  \n    \n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     OH_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n#     Scaler = preprocessing.Normalizer()\n    \n#     X_train[categorical_cols_ordinal] = ordinal_encoder.fit_transform(X_train[categorical_cols_ordinal])\n#     X_valid[categorical_cols_ordinal] = ordinal_encoder.transform(X_valid[categorical_cols_ordinal])\n#     X_test[categorical_cols_ordinal] = ordinal_encoder.transform(X_test[categorical_cols_ordinal])\n    \n#     OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_cols_one_hot]))\n#     OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[categorical_cols_one_hot]))\n#     OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_cols_one_hot]))    \n#     # One-hot encoding removed index; put it back\n#     OH_cols_train.index = X_train.index\n#     OH_cols_valid.index = X_valid.index\n#     OH_cols_test.index = X_test.index    \n#     # Remove categorical columns (will replace with one-hot encoding)\n#     num_X_train = X_train.drop(categorical_cols_one_hot, axis=1)\n#     num_X_valid = X_valid.drop(categorical_cols_one_hot, axis=1)\n#     num_X_test = X_test.drop(categorical_cols_one_hot, axis=1)    \n#     # Add one-hot encoded columns to numerical features\n#     OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n#     OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n#     OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)    \n#     X_train = OH_X_train\n#     X_valid = OH_X_valid\n#     X_test = OH_X_test   \n    \n#     X_train[numerical_cols] = Scaler.fit_transform(X_train[numerical_cols])\n#     X_valid[numerical_cols] = Scaler.transform(X_valid[numerical_cols])\n#     X_test[numerical_cols] = Scaler.transform(X_test[numerical_cols])\n    \n#     model= XGBRegressor(random_state=42,tree_method = 'gpu_hist',gpu_id=0,predictor=\"gpu_predictor\")    \n#     model.fit(X_train, y_train)\n#     preds_valid = model.predict(X_valid)\n#     test_preds = model.predict(X_test)\n#     final_predictions.append(test_preds)\n#     rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n#     print(fold, rmse)\n#     scores.append(rmse)\n\n# print(np.mean(scores), np.std(scores))\n\n\n# 0 0.7386560007376782\n# 1 0.7381008013477749\n# 2 0.7410906745931484\n# 3 0.7399506836365648\n# 4 0.7408779002299951\n# 0.7397352121090323 0.0011853639430475993","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:00:21.900586Z","iopub.execute_input":"2021-08-28T07:00:21.901178Z","iopub.status.idle":"2021-08-28T07:00:21.912735Z","shell.execute_reply.started":"2021-08-28T07:00:21.901128Z","shell.execute_reply":"2021-08-28T07:00:21.911846Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"### Log Transforamtion of numerical values\n\n# data = pd.read_csv(\"../input/30daysofmlfolds/train_folds.csv\")\n# test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n# useful_features = [c for c in data.columns if c not in (\"id\", \"target\", \"kfold\")]\n# categorical_cols_one_hot = [cname for cname in data.columns if data[cname].nunique() < 10 and \n#                         data[cname].dtype == \"object\"]\n# categorical_cols_ordinal = [col for col in data.columns if 'cat' in col and \n#                             col not in categorical_cols_one_hot]\n# numerical_cols = [col for col in useful_features if 'cont' in col]\n\n# test = test[useful_features]\n\n# for col in numerical_cols:\n#     data[col] = np.log1p(data[col])\n#     test[col] = np.log1p(test[col])\n\n# final_predictions = []\n# scores = []\n\n# for fold in range(5):\n#     X_train = data[data.kfold != fold].reset_index(drop = True)\n#     X_valid = data[data.kfold == fold].reset_index(drop = True)\n#     X_test  = test.copy()\n    \n#     y_train = X_train.target\n#     y_valid = X_valid.target\n    \n#     X_train = X_train[useful_features]\n#     X_valid = X_valid[useful_features]  \n    \n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     OH_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)    \n    \n#     X_train[categorical_cols_ordinal] = ordinal_encoder.fit_transform(X_train[categorical_cols_ordinal])\n#     X_valid[categorical_cols_ordinal] = ordinal_encoder.transform(X_valid[categorical_cols_ordinal])\n#     X_test[categorical_cols_ordinal] = ordinal_encoder.transform(X_test[categorical_cols_ordinal])\n    \n#     OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_cols_one_hot]))\n#     OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[categorical_cols_one_hot]))\n#     OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_cols_one_hot]))    \n#     # One-hot encoding removed index; put it back\n#     OH_cols_train.index = X_train.index\n#     OH_cols_valid.index = X_valid.index\n#     OH_cols_test.index = X_test.index    \n#     # Remove categorical columns (will replace with one-hot encoding)\n#     num_X_train = X_train.drop(categorical_cols_one_hot, axis=1)\n#     num_X_valid = X_valid.drop(categorical_cols_one_hot, axis=1)\n#     num_X_test = X_test.drop(categorical_cols_one_hot, axis=1)    \n#     # Add one-hot encoded columns to numerical features\n#     OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n#     OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n#     OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)    \n#     X_train = OH_X_train\n#     X_valid = OH_X_valid\n#     X_test = OH_X_test   \n\n#     model= XGBRegressor(n_estimators = 1000,learning_rate =0.05 , max_depth = 5 ,colsample_bytree=0.5,reg_alpha=15,random_state=42,tree_method = 'gpu_hist',gpu_id=0,predictor=\"gpu_predictor\")    \n#     model.fit(X_train, y_train,early_stopping_rounds=5, \n#              eval_set=[(X_valid, y_valid)], \n#              verbose=False)\n#     preds_valid = model.predict(X_valid)\n#     test_preds = model.predict(X_test)\n#     final_predictions.append(test_preds)\n#     rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n#     print(fold, rmse)\n#     scores.append(rmse)\n\n# print(np.mean(scores), np.std(scores))\n\n\n# # 0 0.7188020378848241\n# # 1 0.7189876396376185\n# # 2 0.721436676934343\n# # 3 0.7210172158021803\n# # 4 0.7199891387231777\n# # 0.7200465417964288 0.0010533795714609726","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:00:21.914071Z","iopub.execute_input":"2021-08-28T07:00:21.914529Z","iopub.status.idle":"2021-08-28T07:00:21.924134Z","shell.execute_reply.started":"2021-08-28T07:00:21.914483Z","shell.execute_reply":"2021-08-28T07:00:21.923405Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"### Polynomial Features\n\n# data = pd.read_csv(\"../input/30daysofmlfolds/train_folds.csv\")\n# test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n# useful_features = [c for c in data.columns if c not in (\"id\", \"target\", \"kfold\")]\n# categorical_cols_one_hot = [cname for cname in data.columns if data[cname].nunique() < 10 and \n#                         data[cname].dtype == \"object\"]\n# categorical_cols_ordinal = [col for col in data.columns if 'cat' in col and \n#                             col not in categorical_cols_one_hot]\n# numerical_cols = [col for col in useful_features if 'cont' in col]\n\n# test = test[useful_features]\n\n# poly = preprocessing.PolynomialFeatures(degree = 2,interaction_only=True,include_bias=False)\n# train_poly = poly.fit_transform(data[numerical_cols])\n# test_poly = poly.transform(test[numerical_cols])\n\n# data_train_poly = pd.DataFrame(train_poly,columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\n# data_test_poly = pd.DataFrame(test_poly,columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\n# data = pd.concat([data,data_train_poly],axis=1)\n# test = pd.concat([test,data_test_poly],axis=1)\n\n# useful_features = [c for c in data.columns if c not in (\"id\", \"target\", \"kfold\")]\n# categorical_cols_one_hot = [cname for cname in data.columns if data[cname].nunique() < 10 and \n#                         data[cname].dtype == \"object\"]\n# categorical_cols_ordinal = [col for col in data.columns if 'cat' in col and \n#                             col not in categorical_cols_one_hot]\n# test = test[useful_features]\n\n\n# final_predictions = []\n# scores = []\n\n# for fold in range(5):\n#     X_train = data[data.kfold != fold].reset_index(drop = True)\n#     X_valid = data[data.kfold == fold].reset_index(drop = True)\n#     X_test  = test.copy()\n    \n#     y_train = X_train.target\n#     y_valid = X_valid.target\n    \n#     X_train = X_train[useful_features]\n#     X_valid = X_valid[useful_features]  \n    \n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     OH_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)    \n    \n#     X_train[categorical_cols_ordinal] = ordinal_encoder.fit_transform(X_train[categorical_cols_ordinal])\n#     X_valid[categorical_cols_ordinal] = ordinal_encoder.transform(X_valid[categorical_cols_ordinal])\n#     X_test[categorical_cols_ordinal] = ordinal_encoder.transform(X_test[categorical_cols_ordinal])\n    \n#     OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_cols_one_hot]))\n#     OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[categorical_cols_one_hot]))\n#     OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_cols_one_hot]))    \n#     # One-hot encoding removed index; put it back\n#     OH_cols_train.index = X_train.index\n#     OH_cols_valid.index = X_valid.index\n#     OH_cols_test.index = X_test.index    \n#     # Remove categorical columns (will replace with one-hot encoding)\n#     num_X_train = X_train.drop(categorical_cols_one_hot, axis=1)\n#     num_X_valid = X_valid.drop(categorical_cols_one_hot, axis=1)\n#     num_X_test = X_test.drop(categorical_cols_one_hot, axis=1)    \n#     # Add one-hot encoded columns to numerical features\n#     OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n#     OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n#     OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)    \n#     X_train = OH_X_train\n#     X_valid = OH_X_valid\n#     X_test = OH_X_test   \n\n#     model= XGBRegressor(n_estimators = 1000,learning_rate =0.05 , max_depth = 5 ,colsample_bytree=0.5,reg_alpha=15,random_state=42,tree_method = 'gpu_hist',gpu_id=0,predictor=\"gpu_predictor\")    \n#     model.fit(X_train, y_train,early_stopping_rounds=5, \n#              eval_set=[(X_valid, y_valid)], \n#              verbose=False)\n#     preds_valid = model.predict(X_valid)\n#     test_preds = model.predict(X_test)\n#     final_predictions.append(test_preds)\n#     rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n#     print(fold, rmse)\n#     scores.append(rmse)\n\n# print(np.mean(scores), np.std(scores))\n\n# # 0 0.7214350186870506\n# # 1 0.7214469969594908\n# # 2 0.7230667556849094\n# # 3 0.7226563873007876\n# # 4 0.7220255877932962\n# # 0.722126149285107 0.000650371394084503","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:00:21.925470Z","iopub.execute_input":"2021-08-28T07:00:21.925920Z","iopub.status.idle":"2021-08-28T07:00:21.936565Z","shell.execute_reply.started":"2021-08-28T07:00:21.925885Z","shell.execute_reply":"2021-08-28T07:00:21.935655Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"### Log Transforamtion on numerical features\n\n# data = pd.read_csv(\"../input/30daysofmlfolds/train_folds.csv\")\n# test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n# useful_features = [c for c in data.columns if c not in (\"id\", \"target\", \"kfold\")]\n# categorical_cols = [col for col in useful_features if 'cat' in col]\n# numerical_cols = [col for col in useful_features if 'cont' in col]\n\n# test = test[useful_features]\n\n# for col in numerical_cols:\n#     data[col] = np.log1p(data[col])\n#     test[col] = np.log1p(test[col])\n\n# final_predictions = []\n# scores = []\n\n# for fold in range(5):\n#     X_train = data[data.kfold != fold].reset_index(drop = True)\n#     X_valid = data[data.kfold == fold].reset_index(drop = True)\n#     X_test  = test.copy()\n    \n#     y_train = X_train.target\n#     y_valid = X_valid.target\n    \n#     X_train = X_train[useful_features]\n#     X_valid = X_valid[useful_features]  \n    \n#     OH_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)    \n    \n#     X_train[categorical_cols] = ordinal_encoder.fit_transform(X_train[categorical_cols])\n#     X_valid[categorical_cols] = ordinal_encoder.transform(X_valid[categorical_cols])\n#     X_test[categorical_cols] = ordinal_encoder.transform(X_test[categorical_cols])\n    \n#     OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_cols]))\n#     OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[categorical_cols]))\n#     OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_cols]))    \n#     # One-hot encoding removed index; put it back\n#     OH_cols_train.index = X_train.index\n#     OH_cols_valid.index = X_valid.index\n#     OH_cols_test.index = X_test.index    \n#     # Remove categorical columns (will replace with one-hot encoding)\n#     num_X_train = X_train.drop(categorical_cols, axis=1)\n#     num_X_valid = X_valid.drop(categorical_cols, axis=1)\n#     num_X_test = X_test.drop(categorical_cols, axis=1)    \n#     # Add one-hot encoded columns to numerical features\n#     OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n#     OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n#     OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)    \n#     X_train = OH_X_train\n#     X_valid = OH_X_valid\n#     X_test = OH_X_test   \n    \n#     model= XGBRegressor(n_estimators = 1000,learning_rate =0.05 , max_depth = 5 ,colsample_bytree=0.5,reg_alpha=15,random_state=42,tree_method = 'gpu_hist',gpu_id=0,predictor=\"gpu_predictor\")    \n#     model.fit(X_train, y_train,early_stopping_rounds=5, \n#              eval_set=[(X_valid, y_valid)], \n#              verbose=False)\n#     preds_valid = model.predict(X_valid)\n#     test_preds = model.predict(X_test)\n#     final_predictions.append(test_preds)\n#     rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n#     print(fold, rmse)\n#     scores.append(rmse)\n\n# print(np.mean(scores), np.std(scores))\n\n\n# 0 0.7192777650589057\n# 1 0.7191080033310232\n# 2 0.720964177329705\n# 3 0.7212998072405337\n# 4 0.7198886187593272\n# 0.7201076743438989 0.0008821400243008312","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:00:21.937914Z","iopub.execute_input":"2021-08-28T07:00:21.938320Z","iopub.status.idle":"2021-08-28T07:00:21.948524Z","shell.execute_reply.started":"2021-08-28T07:00:21.938285Z","shell.execute_reply":"2021-08-28T07:00:21.947535Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# ##Log Transforamtion on numerical and on categorical cols after OneHotEncoding \n# data = pd.read_csv(\"../input/30daysofmlfolds/train_folds.csv\")\n# test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n# useful_features = [c for c in data.columns if c not in (\"id\", \"target\", \"kfold\")]\n# object_cols = [col for col in useful_features if 'cat' in col ]\n# numerical_cols = [col for col in useful_features if 'cont' in col]\n# test = test[useful_features]\n\n# for col in numerical_cols:\n#     data[col] = np.log1p(data[col])\n#     test[col] = np.log1p(test[col])\n\n# final_predictions = []\n# scores = []\n# for fold in range(5):\n#     X_train =  data[data.kfold != fold].reset_index(drop=True)\n#     X_valid = data[data.kfold == fold].reset_index(drop=True)\n#     X_test = test.copy()\n\n#     y_train = X_train.target\n#     y_valid = X_valid.target\n    \n#     X_train = X_train[useful_features]\n#     X_valid = X_valid[useful_features]\n    \n#     oh_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n#     X_train_ohe = oh_encoder.fit_transform(X_train[object_cols])\n#     X_valid_ohe = oh_encoder.transform(X_valid[object_cols])\n#     X_test_ohe= oh_encoder.transform(X_test[object_cols])\n        \n#     X_train_ohe = pd.DataFrame(X_train_ohe,columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])]) \n#     X_valid_ohe = pd.DataFrame(X_valid_ohe,columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])]) \n#     X_test_ohe = pd.DataFrame(X_test_ohe,columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])]) \n    \n#     num_X_train = X_train.drop(object_cols, axis=1)\n#     num_X_valid = X_valid.drop(object_cols, axis=1)\n#     num_X_test = X_test.drop(object_cols, axis=1) \n    \n#     X_train = pd.concat([num_X_train,X_train_ohe],axis=1)\n#     X_valid = pd.concat([num_X_valid,X_valid_ohe],axis=1)\n#     X_test = pd.concat([num_X_test,X_test_ohe],axis=1)\n         \n#     ohe_cols = [col for col in X_train if 'ohe' in col]\n    \n#     for cols in ohe_cols:        \n#         X_train[cols] = np.log1p(X_train[cols])\n#         X_valid[cols] = np.log1p(X_valid[cols])\n#         X_test[col] = np.log1p(X_test[cols])\n    \n        \n#     model = XGBRegressor(n_estimators = 1000,learning_rate =0.05 , max_depth = 5 ,colsample_bytree=0.5,reg_alpha=15,random_state=42,tree_method = 'gpu_hist',gpu_id=0,predictor=\"gpu_predictor\")\n#     model.fit(X_train, y_train)\n#     preds_valid = model.predict(X_valid)\n#     test_preds = model.predict(X_test)\n#     final_predictions.append(test_preds)\n#     rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n#     print(fold, rmse)\n#     scores.append(rmse)\n\n# print(np.mean(scores), np.std(scores))\n\n\n\n# # 0 0.7191230380116586\n# # 1 0.7187730809168179\n# # 2 0.7209006869741788\n# # 3 0.720904536196657\n# # 4 0.7195269731715235\n# # 0.7198456630541672 0.0008953737139768545","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:04:46.321134Z","iopub.execute_input":"2021-08-28T07:04:46.321576Z","iopub.status.idle":"2021-08-28T07:04:46.326344Z","shell.execute_reply.started":"2021-08-28T07:04:46.321540Z","shell.execute_reply":"2021-08-28T07:04:46.325492Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# ###Scalar Standardisation on numerical and on categorical cols after OneHotEncoding \n# data = pd.read_csv(\"../input/30daysofmlfolds/train_folds.csv\")\n# test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n# useful_features = [c for c in data.columns if c not in (\"id\", \"target\", \"kfold\")]\n# object_cols = [col for col in useful_features if 'cat' in col ]\n# numerical_cols = [col for col in useful_features if 'cont' in col]\n# test = test[useful_features]\n\n# final_predictions = []\n# scores = []\n# for fold in range(5):\n#     X_train =  data[data.kfold != fold].reset_index(drop=True)\n#     X_valid = data[data.kfold == fold].reset_index(drop=True)\n#     X_test = test.copy()\n\n#     y_train = X_train.target\n#     y_valid = X_valid.target\n    \n#     X_train = X_train[useful_features]\n#     X_valid = X_valid[useful_features]\n    \n#     oh_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n#     X_train_ohe = oh_encoder.fit_transform(X_train[object_cols])\n#     X_valid_ohe = oh_encoder.transform(X_valid[object_cols])\n#     X_test_ohe= oh_encoder.transform(X_test[object_cols])\n        \n#     X_train_ohe = pd.DataFrame(X_train_ohe,columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])]) \n#     X_valid_ohe = pd.DataFrame(X_valid_ohe,columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])]) \n#     X_test_ohe = pd.DataFrame(X_test_ohe,columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])]) \n    \n#     num_X_train = X_train.drop(object_cols, axis=1)\n#     num_X_valid = X_valid.drop(object_cols, axis=1)\n#     num_X_test = X_test.drop(object_cols, axis=1) \n    \n#     X_train = pd.concat([num_X_train,X_train_ohe],axis=1)\n#     X_valid = pd.concat([num_X_valid,X_valid_ohe],axis=1)\n#     X_test = pd.concat([num_X_test,X_test_ohe],axis=1)\n            \n    \n#     ohe_cols = [col for col in X_train if 'ohe' in col]\n#     Scaler = preprocessing.StandardScaler()        \n#     X_train[ohe_cols] = Scaler.fit_transform(X_train[ohe_cols])\n#     X_valid[ohe_cols] = Scaler.transform(X_valid[ohe_cols])\n#     X_test[ohe_cols] = Scaler.transform(X_test[ohe_cols])    \n        \n#     X_train[numerical_cols] = Scaler.fit_transform(X_train[numerical_cols])\n#     X_valid[numerical_cols] = Scaler.transform(X_valid[numerical_cols])\n#     X_test[numerical_cols] = Scaler.transform(X_test[numerical_cols])\n    \n        \n#     model = XGBRegressor(n_estimators = 1000,learning_rate =0.05 , max_depth = 5 ,colsample_bytree=0.5,reg_alpha=15,random_state=42,tree_method = 'gpu_hist',gpu_id=0,predictor=\"gpu_predictor\")\n#     model.fit(X_train, y_train)\n#     preds_valid = model.predict(X_valid)\n#     test_preds = model.predict(X_test)\n#     final_predictions.append(test_preds)\n#     rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n#     print(fold, rmse)\n#     scores.append(rmse)\n\n# print(np.mean(scores), np.std(scores))\n\n\n# # 0 0.7190107369173784\n# # 1 0.7185684867795665\n# # 2 0.7208843600565435\n# # 3 0.7207950698457429\n# # 4 0.7195472009485909\n# # 0.7197611709095645 0.000934016126723609","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:01:05.526598Z","iopub.execute_input":"2021-08-28T07:01:05.526963Z","iopub.status.idle":"2021-08-28T07:01:05.533379Z","shell.execute_reply.started":"2021-08-28T07:01:05.526925Z","shell.execute_reply":"2021-08-28T07:01:05.532193Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# data = pd.read_csv(\"../input/30daysofmlfolds/train_folds.csv\")\n# test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n# sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n\n# useful_features = [c for c in data.columns if c not in (\"id\", \"target\", \"kfold\")]\n# numerical_cols = [col for col in useful_features if 'cont' in col]\n# ord_cols = ['cat1','cat5','cat8']\n# object_cols = [col for col in useful_features if 'cat' in col and col not in ord_cols]\n# test = test[useful_features]\n\n# final_predictions = []\n# scores = []\n# for fold in range(5):\n#     X_train =  data[data.kfold != fold].reset_index(drop=True)\n#     X_valid = data[data.kfold == fold].reset_index(drop=True)\n#     X_test = test.copy()\n\n#     y_train = X_train.target\n#     y_valid = X_valid.target\n    \n#     X_train = X_train[useful_features]\n#     X_valid = X_valid[useful_features]\n    \n#     oh_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n#     X_train_ohe = oh_encoder.fit_transform(X_train[object_cols])\n#     X_valid_ohe = oh_encoder.transform(X_valid[object_cols])\n#     X_test_ohe= oh_encoder.transform(X_test[object_cols])\n        \n#     X_train_ohe = pd.DataFrame(X_train_ohe,columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])]) \n#     X_valid_ohe = pd.DataFrame(X_valid_ohe,columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])]) \n#     X_test_ohe = pd.DataFrame(X_test_ohe,columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])]) \n    \n#     num_X_train = X_train.drop(object_cols, axis=1)\n#     num_X_valid = X_valid.drop(object_cols, axis=1)\n#     num_X_test = X_test.drop(object_cols, axis=1) \n    \n#     X_train = pd.concat([num_X_train,X_train_ohe],axis=1)\n#     X_valid = pd.concat([num_X_valid,X_valid_ohe],axis=1)\n#     X_test = pd.concat([num_X_test,X_test_ohe],axis=1)            \n    \n#     ohe_cols = [col for col in X_train if 'ohe' in col]\n#     Scalar = preprocessing.StandardScaler()        \n#     X_train[ohe_cols] = Scalar.fit_transform(X_train[ohe_cols])\n#     X_valid[ohe_cols] = Scalar.transform(X_valid[ohe_cols])\n#     X_test[ohe_cols] = Scalar.transform(X_test[ohe_cols])    \n        \n#     X_train[numerical_cols] = Scalar.fit_transform(X_train[numerical_cols])\n#     X_valid[numerical_cols] = Scalar.transform(X_valid[numerical_cols])\n#     X_test[numerical_cols] = Scalar.transform(X_test[numerical_cols])\n    \n#     ordinal_encoder = preprocessing.OrdinalEncoder()\n#     X_train[ord_cols] = ordinal_encoder.fit_transform(X_train[ord_cols])\n#     X_valid[ord_cols] = ordinal_encoder.transform(X_valid[ord_cols])\n#     X_test[ord_cols] = ordinal_encoder.transform(X_test[ord_cols])\n    \n#     X_train[ord_cols] = Scalar.fit_transform(X_train[ord_cols])\n#     X_valid[ord_cols] = Scalar.transform(X_valid[ord_cols])\n#     X_test[ord_cols] = Scalar.transform(X_test[ord_cols])            \n        \n#     model = XGBRegressor(n_estimators = 1000,learning_rate =0.05 , max_depth = 5 ,colsample_bytree=0.5,reg_alpha=15,random_state=42,tree_method = 'gpu_hist',gpu_id=0,predictor=\"gpu_predictor\")\n#     model.fit(X_train, y_train)\n#     preds_valid = model.predict(X_valid)\n#     test_preds = model.predict(X_test)\n#     final_predictions.append(test_preds)\n#     rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n#     print(fold, rmse)\n#     scores.append(rmse)\n\n# print(np.mean(scores), np.std(scores))\n\n# # 0 0.7191607665647867\n# # 1 0.7187366147644717\n# # 2 0.7207301568253949\n# # 3 0.7209203652063809\n# # 4 0.7195942225943873\n# # 0.7198284251910844 0.0008600141676218628","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:01:05.535109Z","iopub.execute_input":"2021-08-28T07:01:05.535518Z","iopub.status.idle":"2021-08-28T07:01:05.548947Z","shell.execute_reply.started":"2021-08-28T07:01:05.535481Z","shell.execute_reply":"2021-08-28T07:01:05.548096Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"preds = np.mean(np.column_stack(final_predictions),axis=1)\nsample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\nsample_submission.target = preds\nsample_submission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T07:01:05.550394Z","iopub.execute_input":"2021-08-28T07:01:05.550895Z","iopub.status.idle":"2021-08-28T07:01:06.105657Z","shell.execute_reply.started":"2021-08-28T07:01:05.550858Z","shell.execute_reply":"2021-08-28T07:01:06.104760Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}